{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Exam\n",
    "\n",
    "Based on the lecture materials, the following problems are designed to cover your initial learning of python and onto applications of the various ML techniques with a toy datasets that you will first form.\n",
    "\n",
    "The necessary imports are provided for you, however we recommend looking up each of the libraries / packages as you encounter them to help you with the answers. Understanding and reading the application of these packages is a fundamental aspect of writing efficent code. We recommend using the # key to annotate your code as this will make it easier for you to come back to the problems.\n",
    "\n",
    "Please hand in the HTML and IPYNB files of your answers.\n",
    "\n",
    "Parts of the notebook have been set up in advance for you - do not change these or you will only complicate your work. You are expected to fill in the blanks on the provided code, such that the cell will run and achieve its task. Most of the questions will require you to complete earlier questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports for your test. We will test that these all work before you start your exam. \n",
    "\n",
    "#%matplotlib notebook\n",
    "#Standard imports for data science with python\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "#Imports for plotting using matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "plt_style = 'fivethirtyeight'\n",
    "\n",
    "#Imports for advanced visualisation / analytical methods\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a toy dataset to work with\n",
    "\n",
    "step 1: Create the data using sklearns make_classification (We have provided this for you already). Print the shape of the generated data. The independent variables (input data also called features) are the X. The dependent variables are the y.\n",
    "\n",
    "step 2: Save the data as an npy files: X.npy and y.npy for generated X and y data.\n",
    "\n",
    "step 3: Use kernel density estimation on the X data (you don't have to understand this to use it - this is just to see how well you can handle unknown models. This means you will need to read the documentation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: make an X and y dataset with 2 features that classifies the y as a binary 1 or 0\n",
    "# This is provided for you already:\n",
    "X, y = make_classification(n_samples=100, \n",
    "                           n_features=3, \n",
    "                           n_informative=2, \n",
    "                           n_redundant=0, \n",
    "                           n_repeated=0, \n",
    "                           n_classes=2, \n",
    "                           n_clusters_per_class=1, \n",
    "                           class_sep=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of X and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: save the data using np.save(). Check the documentation to see how to do it. Remember that a string (str) needs ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the X data as an npy file called X.npy\n",
    "\n",
    "# save the y data as an npy file called y.npy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Plotting and interpreting the data using kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an appropriate kde plot from seaborn to visualise one the first(zeroth in python) dimension (univariate) and then try two (bivariate). \n",
    "\n",
    "**Recall slicing array data, \":\" denotes taking all the data in that axis, for example X[:,1] would take all the data in the 2nd axis**\n",
    "\n",
    "There are blank spaces for you to fil in kdeplot on 2 lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-2cc8623d95d9>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-2cc8623d95d9>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    ax = sns.kdeplot( , , shade=True) # fill in here 2 dimensions of your choice input data\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Univariate\n",
    "sns.kdeplot( ) # fill in here 1 dimension of the input data\n",
    "plt.show()\n",
    "#Bivariate\n",
    "ax = sns.kdeplot( , , shade=True) # fill in here 2 dimensions of your choice input data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis\n",
    "\n",
    "Step 1: Scale the data using StandardScaler\n",
    "\n",
    "Step 2: Run the PCA algorithm from sklearn\n",
    "\n",
    "Step 3: Plot the Principle Components\n",
    "\n",
    "**Note you don't have to understand this theory, just read the documentation https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit_transform() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ceeea18f7345>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# fill in this line with the appropriate variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# lets check it worked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit_transform() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "\n",
    "scaler = StandardScaler() \n",
    "X_scale = scaler.fit_transform( ) # fill in this line with the appropriate variable\n",
    "\n",
    "# lets check it worked\n",
    "for i in range(3):\n",
    "    print(f\"mean of {np.mean(X_scale[:,i])}, standard deviation of  {np.std(X_scale[:,i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ee6e8082c420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# initialize with n = 2 components, read the docs if you need to find the right keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# fill in here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "# Run PCA with 2 components\n",
    "\n",
    "pca = PCA( ) # initialize with n = 2 components, read the docs if you need to find the right keywords\n",
    "pca.fit( ) # fill in here\n",
    "\n",
    "plt.bar(list(range(1,4)), pca.explained_variance_ratio_)\n",
    "plt.xlabel('PCs')\n",
    "plt.ylabel('%')\n",
    "plt.ylim(0,1)\n",
    "plt.title('explained variance ratios')\n",
    "print('The sum of the explained variance ratios is:' , sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# run the pca on the X_scale data\n",
    "X_pca = pca.fit_transform() # fill in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Clustering\n",
    "\n",
    "Because we made the dataset ourselves, we know there are 2 clusters in here, so please confirm this using KMeans clustering. \n",
    "\n",
    "**https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-06d397195ef9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-06d397195ef9>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    kmeans = KMeans( , random_state=0) # initialize the kmeans model with 2 clusters (check the documentation for keywords)\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# run kmeans\n",
    "kmeans = KMeans( , random_state=0) # initialize the kmeans model with 2 clusters (check the documentation for keywords)\n",
    "kmeans.fit( ) # fill in here with pca transformed X data\n",
    "\n",
    "# plot the first 2 PCs\n",
    "# hint: the first PC looks like this: X_pca[:,0]\n",
    "plt.scatter( , ,c=kmeans.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Step 1: choose the correct model to classify the the data\n",
    "\n",
    "Step 2: Train_test_split the data, the test size should be set at 33%\n",
    "\n",
    "Step 3: Train the model on the train data\n",
    "\n",
    "Step 4: Validate the model with test data using the \"score\" function\n",
    "\n",
    "Step 5: Discuss metrics for measuring how useful these models would be. Name at least 2 ways to measure the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Choose the model\n",
    "# IMPORT HERE: which will look somethin like from ___ import ____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-6f22022bc69e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-6f22022bc69e>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    clf = # Choose the model here to instantiate. Read the docs to see if you need any initialization but this is not required unless the model you choose has specific user inputs.\u001b[0m\n\u001b[1;37m                                                                                                                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "clf = # Choose the model here to instantiate. \n",
    "# NOTE: Read the docs to see if you need any initialization but this is not required unless the model you choose has specific user inputs.\n",
    "clf.fit( )# fill in here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4, find the score value from the held back test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5, name at least 2 other metrics for this model we could use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Functions putting it all together\n",
    "Make the code below into a function that forms a classification analysis pipeline for what has come before. Remember that there should be 4 space indents after the function defintion:\n",
    "\n",
    "The input arguments should be the string file names for the X.npy and y.npy files and the outputs (returns) should be the model_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-16-a598a98f0702>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-a598a98f0702>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    X = np.load(X_file)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# fill in here\n",
    "    X = np.load(X_file)\n",
    "    y = np.load(y_file)\n",
    "    X_scale = # Fill in here from \n",
    "    X_train, X_test, y_train, y_test = train_test_split() # Fill in here\n",
    "    clf.fit() # Fill in hre\n",
    "    model_score = clf.score() # Fill in here\n",
    "    return # fill in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7de20ec71a23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Try it out here:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"X.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"y.npy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Try it out here:\n",
    "X_file, y_file = \"X.npy\", \"y.npy\"\n",
    "pipeline(X_file, y_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
